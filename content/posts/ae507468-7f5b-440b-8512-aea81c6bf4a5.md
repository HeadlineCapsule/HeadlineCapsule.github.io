---
title: “模型崩溃”问题：人类数据不足如何限制了人工智能的发展
date: 2024-07-24T15:00:40.468Z
description: Research suggests use of computer-made ‘synthetic data’ to train top AI models could lead to nonsensical results in future
tags: 
- technology
author: ft
---

[原文链接](https://ft.com/content/ae507468-7f5b-440b-8512-aea81c6bf4a5)

“模型崩溃”问题：人类数据不足如何限制了人工智能的发展

# 新研究揭示了训练大型语言模型时使用合成数据的风险

**摘要：** 《自然》杂志上发表的一项新研究强调了使用合成数据训练大型语言模型（LLM）的担忧，因为存在一种称为“模型崩溃”的现象。这种情况会导致 AI 模型随着时间的推移而退化，产生胡言乱语。该研究发现，使用计算机生成的数据可能会迅速恶化模型性能，并加剧诸如损失方差和重复短语等问题。

**要点：**

1. **合成数据风险：** 像 OpenAI 和微软这样的领先 AI 公司一直在使用合成数据训练 LLM，因为人类生成材料存在局限性。然而，研究表明这种方法可能会导致 AI 模型产生错误结果。

2. **模型崩溃现象：** 该研究探讨了 AI 模型如何随着时间的推移而崩溃，因为错误在训练的每一代中累积并放大。这会导致方差损失，即大多数子人群变得过度代表，而少数群体被牺牲，最终导致胡言乱语输出。

3. **例子：** 在一次使用关于中世纪建筑的合成输入文本的实验中，AI 模型在不到九代的训练后就开始讨论野兔。另一个例子是，训练用于狗品种图像的 AI 最初专注于金毛寻回犬等常见品种，但最终生成了这些狗的扭曲图像。

4. **缓解挑战：** 研究人员发现缓解这个问题具有挑战性。嵌入水印以标记 AI 生成的内容并将其从训练数据集中排除需要技术公司之间的协调。这可能不切实际或在商业上不可行。

5. **先发优势：** 该研究表明，在构建生成式 AI 模型方面存在先发优势，因为拥有预 AI 互联网数据的公司可能比那些依赖合成数据的公司更好地代表现实世界。

---

 **Summary:** New research published in Nature highlights concerns about using synthetic data to train large language models (LLMs) due to a phenomenon known as 'model collapse.' This occurs when AI models degrade over time, producing nonsensical results. The study found that the use of computer-generated data can lead to rapid deterioration in model performance and exacerbate issues like loss of variance and repetition of phrases.

**Key Points:**
1. **Synthetic Data Risks:** Leading AI companies, such as OpenAI and Microsoft, have been using synthetic data to train LLMs due to the limitations of human-generated material. However, research suggests that this approach may result in erronenous outcomes for AI models.
2. **Model Collapse Phenomenon:** The study explores how AI models can collapse over time as mistakes accumulate and amplify through successive generations of training. This leads to a loss of variance, where majority subpopulations become over-represented at the expense of minority groups, eventually resulting in gibberish output.
3. **Examples:** In one experiment using synthetic input text about medieval architecture, the AI model descended into discussing jackrabbits after fewer than nine generations. Another example showed an AI trained on dog breed images initially focusing on common breeds like golden retrievers and eventually producing distorted images of these dogs.
4. **Challenges in Mitigation:** The researchers found that mitigating the problem has been challenging, with techniques such as embedding watermarks to flag AI-generated content for exclusion from training data sets requiring coordination between technology companies. This may not be practical or commercially viable.
5. **First-Mover Advantage:** The study suggests that there is a first-mover advantage in building generative AI models, as companies with access to pre-AI internet data might have better representations of the real world compared to those relying on synthetic data.

[Source Link](https://ft.com/content/ae507468-7f5b-440b-8512-aea81c6bf4a5)

